---
title: "SORT-seq dataset with velocity preparations"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

## *Setting Parameters:* 
To keep in mind: 
- Make sure the plates to combine all have the same amount of "_" separated fields in their folder names. These fields will be used to set up the phenodata columns. - The Combined ID per plate, will be used for labelling in figures.

```{r}
## Parameters for processing dataset and metadata ##
barcode_file = "/scratch/snabel/scrna/barcode_384.tab"
MT_genes_file = "/scratch/snabel/scrna/genomes/genome_addons/MT_genes.txt"
# Edit plate names with substitutes:
old_col_pattern = "_S"
new_col_pattern = "-S"
# Retrieve the variables from the platenames (fields seperated with "_"):
plate_variables = c("Genome", "Spike-in", "Reporter","Cell_type", "Timepoint", "Library", "Index", "Cell_id")
# Unique combined ID per plate, for visualization purposes
combined_variables_to_id = c("Cell_type", "Timepoint", "Library")

## Filtering of the dataset ##
# Settings for genes
gene_tresh = 1
amount_cells_expr = 1
# Settings for cells
total_counts_tresh = 1000
total_feat_tresh = 500
ERCC_pct_max <- 20
mt_pct_max <- 50

## Subsetting the dataset ##
# Specify a specific (sub)string to select the cells on (selection happens on the colnames)
subset_id = ""
# Select the type of filtering: keep cells with the substring (set "in") or remove (set "out")
# if NO filtering is wanted, leave empty (set "" or "no")
filtering = "no"

## Seurat Normalization, HVG selection (vst) & Scaling (and Regression) ##
nHVG = 2000
# Regression performed on the following variables:
vars_to_regress = c("nCount_sf", "nFeature_sf") # If no regression desired: NULL

## Dimensionality reduction ##
# For PCA to run on
pcs_max = 70 
# PCs used for different UMAP representations
pcs_for_overview = c(5, 10, 12, 15, 16, 18, 20, 25, 30, 40)

## Visualization ##
# label from phenodata to color in Scater and Seurat plots
lab_col = "Library"
umap_col = "Cell_type"
umap_col2 = "Timepoint"
# Checking variability explained by confounding factors
confounders_to_test = c("Cell_type", "Timepoint", "Library")
# Marker genes for violin plots
explore_violin = c("KLF4", "SOX2", "MYC", "OCT4", "GAPDH")

## Storing results ##
workdir <- "/path/R_analysis"
# if regression is performed: this will already be included in the folder name
result_descript = "_results_PreprocessingDataset_complete_clean"

## Location of scripts ##
source("/scratch/snabel/R-scripts/scRNA-seq/read_kb_dataset_s2s.R")
source("/scratch/snabel/R-scripts/scRNA-seq/qc_umis_384plot.R")
source("/scratch/snabel/R-scripts/scRNA-seq/qc_ercc_384plot.R")
```

```{r "setup"}
knitr::opts_knit$set(root.dir = normalizePath(workdir))
```

```{r, include = FALSE}
# Loading the important repositories # 
require("devtools")
library(ggplot2)
library(dplyr)
library(tidyr)
library(mvoutlier)
library(limma)
library(knitr)
library(SingleCellExperiment)
library(scater)
library(Seurat)
library(scran)
library(RColorBrewer)
library(plot3D)
# options(stringsAsFactors = FALSE)
```


## *Cleaning up the counts table, for subset: `r subset_id`*

----------------------------------------------------------------------

### Creating the SingleCellExperiment object

The counts table is loaded along with the metadata of the cells within an Scater usable object. Scater will be used to look into the quality of the data and to help with filtering out bad cells or genes.

Location of the file:
```{r loading dataset, echo = TRUE, warning = FALSE}
## Splice seperated dataset:
spliced.data.df = read_kb_counts("../results_human/kallistobus/", "spliced", barcode_file = barcode_file)
unspliced.data.df = read_kb_counts("../results_human/kallistobus/", "unspliced", barcode_file = barcode_file)

## Optional edits on cell names:
# This only runs if a substring that needs replacement was defined in parameters (old_col_pattern):
if (old_col_pattern != ""){
  colnames(spliced.data.df) <- gsub(old_col_pattern, new_col_pattern, colnames(spliced.data.df))
  colnames(unspliced.data.df) <- gsub(old_col_pattern, new_col_pattern, colnames(unspliced.data.df))
}
```

#### Setting up results directory

```{r}
# if regression will be performed, this is included in the folder name.
if (!is.null(vars_to_regress)){
result_descript <- paste(result_descript, "regressed", sep = "_")  
}

dateoftoday <- gsub("-", "", as.character(Sys.Date()))
resultsdir <- paste0(workdir, dateoftoday, "_", result_descript)
system(paste("mkdir -p ", resultsdir))
```

```{r}
knitr::opts_knit$set(root.dir = normalizePath(resultsdir))
```

Results will be stored in:
`resultsdir`

```{r}
# Percentage of reads unspliced
sum(unspliced.data.df)/(sum(spliced.data.df)+sum(unspliced.data.df))

# unspliced.data.df <- as.matrix(unspliced.data.df)
# spliced.data.df <- as.matrix(spliced.data.df)
# Make columnnames the same (order) between matrices

all_cells <- intersect(colnames(spliced.data.df),colnames(unspliced.data.df))

unspliced.data.df <- unspliced.data.df[,all_cells]
spliced.data.df <- spliced.data.df[,all_cells]

identical(colnames(spliced.data.df),colnames(unspliced.data.df))

# The default data.df will be the spliced dataset (shorter to type)
data.df <- spliced.data.df
```

### Perform subsetting of dataset (optional)

```{r filtering certain entries}

if (filtering == "out"){
  # checking the amount of cells in the dataset before filtering
  length(colnames(data.df))
  # the amount of cells you retrieve with the filter.
  length(colnames(data.df[,!grepl(subset_id, colnames(data.df)) == TRUE]))
  # filter cells based on the substring.
  data.df <- data.df[,!grepl(subset_id, colnames(data.df)) == TRUE]
} else if (filtering == "in"){
  # checking the amount of cells in the dataset before filtering
  length(colnames(data.df))
  # the amount of cells you retrieve with the filter.
  length(colnames(data.df[,grepl(subset_id, colnames(data.df)) == TRUE]))
  # filter cells based on the substring.
  data.df <- data.df[,grepl(subset_id, colnames(data.df)) == TRUE]
} else {
  print(paste0("No filtering applied. The amount of cells in the dataset remain: ", as.character(length(colnames(data.df)))))
}

spliced.data.df <- data.df
subset_cells <- intersect(colnames(spliced.data.df), colnames(unspliced.data.df))
unspliced.data.df <- unspliced.data.df[,subset_cells]

# no. of plates:
length(colnames(spliced.data.df))/384
```


```{r phenotable}
## Setting up the phenotable ##
phenodata <- data.frame(row.names=colnames(data.df))
phenodata$names <- row.names(phenodata)
phenodata <- separate(phenodata, col = "names", into = plate_variables, sep = "_")

## Replace by tinyverse using the columns mentioned with combined_variables_to_id
phenodata$combined_id <- apply(phenodata[,combined_variables_to_id], 1, paste, collapse = "_")

# Only take the entries that are matchable with the counttable entries:
pheno_matched <- phenodata[rownames(phenodata) %in% colnames(data.df),]

# Matching phenodata with the dataset ordering
pheno_ordered <- pheno_matched[match(colnames(data.df),rownames(pheno_matched)),]
identical(rownames(pheno_ordered), colnames(spliced.data.df))
write.csv(phenodata, "phenodata_kbordered.csv")
```

## Plate Overviews

Running QC over the plates: are there 

```{r}
## Running plate QC: are there certain patterns?

# Make a list of cell-names compatable with the excel file: plate#_A1, plate#_A2 etc.
plate_order <- read.table(barcode_file, sep = "\t", col.names = c("well","barcode"))

# Make a vector with all plate numbers
platenrs <- unique(gsub("([^_]*)$", "", colnames(data.df)))
pdf("PlateDiag_lndscp.pdf", paper = "USr")
# settings for the plate diagnostics pdf 
par(mfrow=c(2,2), mar = c(5,4,4,2) + 0.1, cex.main = 1)

# Iterate over all plates, order cells in the order of visualization
for (plate in platenrs){
  # use the order of cells from te barcode file (this is A1, A2, A3, etc to P24)
  primer_order <- paste(plate, plate_order$well, sep="")
  
  # if wells are missing on the plate, these need to be added and given a value of 0
  missing_wells <- primer_order[!primer_order %in% colnames(spliced.data.df)]
  cols_to_add <- data.frame(matrix(ncol = length(missing_wells), nrow = length(rownames(spliced.data.df))))
  colnames(cols_to_add) <- missing_wells
  cols_to_add[is.na(cols_to_add)] <- 0
  diag_plate <- cbind(spliced.data.df[,grep(plate, colnames(spliced.data.df))], cols_to_add)
  # phenodata contains same cellid entry + rowname as used in dataset
  cells_order <- colnames(diag_plate[,match(primer_order, colnames(diag_plate))])
  
  # match dataset cells order with wells in the visualization
  tmp <- as.matrix(diag_plate[,cells_order])
  QC_umis_384plot(tmp, paste(plate, "UMI_QC", sep = "_"))
  QC_ERCC_384plot(tmp[grep("^ERCC", rownames(diag_plate)),], paste(plate, "ERCC_QC", sep = "_"))
  
  rm(tmp)
}
dev.off()
```

#### Creating a SingleCellExperiment object for confounder check

```{r build SCE}
# df -> matrix + phenodata -> SCE 
count_matrix <- as.matrix(data.df)
sce <- SingleCellExperiment(assays = list(counts = count_matrix), colData = pheno_ordered, rowData = rownames(count_matrix))
```

```{r filtering empty entries, echo = FALSE}
# Checking if the dataset contains genes without a symbol name:
missing.name <- rownames(sce[is.na(rownames(counts(sce)))])
print(missing.name)
```

## Cleaning the expression matrix

Setting thresholds for the removal of genes too lowly expressed in too few cells.

```{r}
MT_genes <- read.table(MT_genes_file)[,1]

# Adding spike-in information:
isSpike(sce, "ERCC") <- grepl("^ERCC-", rownames(sce))
isSpike(sce, "MT") <- rownames(sce)[rownames(sce) %in% MT_genes]

# Calculate the quality metrics:
sce <- calculateQCMetrics(
  sce, feature_controls = list(
    ERCC = isSpike(sce, "ERCC"), 
    MT = isSpike(sce, "MT")
    )
  )
```

#### Distribution of counts per cell in the dataset

Manually setting arbitrary count tresholds for the cells considered healthy.

```{r}
# Looking at the total number of RNA molecules per sample
# UMI counts were used for this experiment
hist(sce$total_counts, breaks = 100)
abline(v = total_counts_tresh, col = "red")

# Looking at the amount of unique genes per sample
# This is the amount with ERCC included.
hist(sce$total_features_by_counts, breaks = 100)
abline(v= total_feat_tresh, col = "red")
```

Histogram showing the total amounts of counts (x-axis) per proportion of cells (each bar). Red line at: `r total_counts_tresh` counts. 
Histogram showing the total amounts of genes (features) per proportion of cells. Red line at: `r total_feat_tresh` genes.

#### Plotting spike-in data

Spike-ins and mitochondrial expression are used as another measure for quality of the cells. A overrepresentation of spikes and mitochondrial transcript might indicate a "unhealthy" cell or poor library. These plots show the percentage of spike-ins against the total amount of reads that are found in each cell. 

A higher percentage of spike-in indicates a lower amount of endogenous genes found in the cell or in case of mitochondrial genes, a cell that was apoptotic. Also cells that are smaller will have relatively more spike-in allocated reads, and some cell types might have higher numbers of mitochondria, which is important to consider setting this boundary.

```{r}
# Removal of cells causing a warning:
NaN_cells <- unique(c(colnames(sce)[sce$pct_counts_ERCC == "NaN"], colnames(sce)[sce$pct_counts_MT == "NaN"]))
sce <- sce[,!colnames(counts(sce)) %in% NaN_cells]

# Using Scater to plot percentages of spikes
plotColData(sce,
            x = "total_features_by_counts", 
            y = "pct_counts_MT", colour = lab_col)

plotColData(sce,
            x = "total_features_by_counts", 
            y = "pct_counts_ERCC", colour = lab_col)

multiplot(
  plotColData(sce, y="total_counts", x="Library"),
  plotColData(sce, y="total_features_by_counts", x="Library"),
  plotColData(sce, y="pct_counts_ERCC", x="Library"),
  plotColData(sce, y="pct_counts_MT", x="Library"),
  cols=2)

```

Plotting the percentages of the spike-ins against the total amount of genes, each dot represents a cell. Color labelled on `r lab_col`.

#------------------------
## Filtering the cells ##
#------------------------

Using manual thresholds for filtering out the outliers in the dataset.

```{r}
#---------------------------
## Manually set thresholds for filtering of the cells:
#---------------------------
# Filter library-size and the total amount of genes on the thresholds shown above in the histogram.
filter_by_expr_features <- sce$total_features_by_counts >= total_feat_tresh
filter_by_total_counts <- sce$total_counts >= total_counts_tresh
filter_by_ercc <- sce$pct_counts_ERCC < ERCC_pct_max
filter_by_mt <- sce$pct_counts_MT < mt_pct_max

sce$use <- (filter_by_expr_features 
         & 
           filter_by_total_counts 
         &
           filter_by_ercc 
         & 
           filter_by_mt
           )

# Amount of cells removed per filtering:
table(filter_by_expr_features)
table(filter_by_total_counts)
table(filter_by_ercc)
table(filter_by_mt)

# Result of manual filtering with set tresholds
# TRUE are considered healthy cells:
table(sce$use)

```

```{r filter cells}
# The quality check-passing cells are stored in the SCE-object in $use selection of the counts table. 
# Create the quality-checked dataset:
sce_qc <- sce[, colData(sce)$use]
dim(sce_qc)
```

#------------------------
## Filtering the genes ##
#------------------------

```{r filter genes}
#---------------------------
## Filtering the genes
#---------------------------
# You do the filtering of the genes after selecting the healthy cells, because some genes might only be detected in poor quality cells

# UMI were used to collapse the reads of the same transcript
# Filtering on genes considered expressed: above a treshold for a set amount of cells:
keep_feature <- rowSums(counts(sce_qc) >= gene_tresh) >= amount_cells_expr

# Create the quality-checked dataset:
sce_qc <- sce_qc[keep_feature,]

genes_expressed <- sum(keep_feature==TRUE)

write.table("spliced_qc_counts.tsv", col.names = NA, quote = FALSE)
saveRDS(sce_qc, file = "spliced_qc_counts.rds")
```

Plotting the distributions of the dataset before and after filtering.

```{r filtered dataset: compare before/after filtering}
pdf("Histograms_before+aftercellsFiltering.pdf")
par(mfrow=c(2,2))
hist(sce$total_counts, breaks = 100)
abline(v = total_counts_tresh, col = "red")

hist(sce$total_features_by_counts, breaks = 100)
abline(v= total_feat_tresh, col = "red")

hist(sce_qc$total_counts, breaks = 100)
abline(v = total_counts_tresh, col = "red")

hist(sce_qc$total_features_by_counts, breaks = 100)
abline(v= total_feat_tresh, col = "red")
dev.off()

pdf("MT+ERCC_before+aftercellsFiltering.pdf")
par(mfrow=c(2,2))
plotColData(sce,
            x = "total_features_by_counts", 
            y = "pct_counts_MT", colour = lab_col)

plotColData(sce,
            x = "total_features_by_counts", 
            y = "pct_counts_ERCC", colour = lab_col)
plotColData(sce_qc,
            x = "total_features_by_counts", 
            y = "pct_counts_MT", colour = lab_col)

plotColData(sce_qc,
            x = "total_features_by_counts", 
            y = "pct_counts_ERCC", colour = lab_col)
dev.off()

``` 

In the dataset `r genes_expressed` are considered expressed.

```{r endogenous dataset for confounding factors}
#------------------------------
# Filter endogenous genes
#------------------------------
# load the filtered dataset:
#sce_qc <- readRDS("qc_counts.rds")

endo_genes <- !rowData(sce_qc)$is_feature_control
table(endo_genes)

# Make a object with only the endogenous genes to look for confounders
sce_endo <- sce_qc[endo_genes,] 
reducedDim(sce_qc) <- NULL

# https://www.bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/vignette-qc.html#identifying-outliers-on-all-qc-metrics
plotExprsFreqVsMean(sce_endo)

```

```{r}
# The reads consumed by the top 50 expressed genes:
plotHighestExprs(sce_qc)

```

Summary of filtering genes:
Genes that had less than `r amount_cells_expr` cells with an expression less than `r gene_tresh`.
For the genes in this dataset genes that were removed `r table(keep_feature)` genes were kept.
Spikes: `r spikeNames(sce)` were saved in the dataset and used for quality metrics calculations.

## Check for confounding factors

PCA on only the endogenous genes is used to evaluate the influence of the confounding factors.

```{r PCA on raw data}
# The PCA per default uses the top 500 most variable genes of the dataset, changeable with the argument: ntop = 500.
# Plotting the raw data without any transformation.
sce_endo <- runPCA(
  sce_endo,
  ncomponents = 50,
  exprs_values = "counts" 
)
plotReducedDim(sce_endo, use_dimred = "PCA",   
               colour_by = lab_col,
               size_by = "total_features_by_counts")

# The PCA data is stored in the reducedDimNames as a "PCA_coldata" entry, if use_coldata = TRUE in runPCA(). If use_coldata = FALSE, this will be stored in "PCA". 
reducedDimNames(sce_endo)

```

# Raw log2-transformation
To compare with other normalization methods.

```{r raw log2-transformation}
assay(sce_endo, "logcounts_raw") <- log2(counts(sce_endo) + 1)

# plotReducedDim and plotPCA will do the same, with plotPCA you leave out the use_dimred="PCA" argument.
tmp <- runPCA(sce_endo, ncomponents = 50, exprs_values = "logcounts_raw")
# plot PCA after log2 transformation
plotPCA(tmp, 
        colour_by = lab_col,
        size_by = "total_features_by_counts")
# One can also run tSNE in similar ways with Scater.
rm(tmp)
# The logcounts_raw is not enough to account for the technical factors between the cells.
```

## Normalization in Seurat
Make the seurat object, 'seuset'.
In this step you could filter the cells again, these however already have been filtered before in 'table clean-up', where the genes were taken that have >2 cells that have an expression >1.

```{r create seurat object}
seuset <- CreateSeuratObject(counts = counts(sce_endo), assay = "sf", meta.data = as.data.frame(colData(sce_endo)[,1:(length(plate_variables)+1)]))

```

```{r}
# looking into the dataset
VlnPlot(
    object = seuset, 
    features = c("nFeature_sf"), 
    group.by = lab_col
)
VlnPlot(
    object = seuset, 
    features = c("nCount_sf"), 
    group.by = lab_col
)
VlnPlot(
  object = seuset, 
    features = explore_violin, 
    group.by = lab_col
)
FeatureScatter(
    object = seuset, 
    feature1 = "nCount_sf", 
    feature2 = "nFeature_sf"
)

```

```{r}
# Seurat normalization: "a global-scaling normalization method LogNormalize that normalizes the gene expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result.""
seu <- seuset
seuset <- NormalizeData(
    object = seuset, 
    normalization.method = "LogNormalize", 
    scale.factor = 10000
)
# looking into the dataset
VlnPlot(
    object = seuset, 
    features = c("nFeature_sf"), 
    group.by = lab_col
)
VlnPlot(
    object = seuset, 
    features = c("nCount_sf"), 
    group.by = lab_col
)
VlnPlot(
  object = seuset, 
    features = explore_violin, 
    group.by = lab_col
)
FeatureScatter(
    object = seuset, 
    feature1 = "nCount_sf", 
    feature2 = "nFeature_sf"
)
saveRDS(seuset, "seuset_qc+norm.rds")
```

#### Check confounders before & after normalization

```{r seurat objects to sce} 
# Only take the entries that are matchable with the counttable entries:
filtered_cells <- intersect(rownames(pheno_ordered), colnames(seuset@assays$sf@data))
pheno_matchedseuset <- phenodata[filtered_cells,]
pheno_orderedseuset <- pheno_matchedseuset[match(colnames(seuset@assays$sf@data),rownames(pheno_matchedseuset)),]

count_matrixseuset <- as.matrix(seuset@assays$sf@data)

sce_seunorm <- SingleCellExperiment(assays = list(counts = count_matrixseuset), colData = pheno_orderedseuset, rowData = rownames(count_matrixseuset))

# A little trick to let scater know that there are actually logcounts in the dataset.
assay(sce_seunorm, "logcounts") <- counts(sce_seunorm)

# Calculate the quality metrics:
sce_seunorm <- calculateQCMetrics(
  sce_seunorm)

```

# Identifying the variation caused by each confounding factor
#### Before & after normalization 

```{r check confounders in raw dataset}

explanatory_variables <- as.factor(c(confounders_to_test, "total_features_by_counts", "total_counts"))
#explanatory_variables_seu <- c("total_features_by_counts", "total_counts", confounders_to_test)

# This function and visualization performs a PCA analysis in the data object and checks to what extend the variables that are put in, are explaining the variance.
# The percentage of variance explained by each variable of interest:

# Setting the colours:
colourvector <- c()
colourset <- brewer.pal(length(explanatory_variables),"Dark2")
i <- 1
for (variable_item in explanatory_variables){
  colourvector[variable_item] <- colourset[i]
  i <- i + 1 
}

# Building combined plot, before and after normalization
p1 <- plotExplanatoryVariables(sce_endo, 
                         exprs_values = "counts",
                         variables = explanatory_variables) + expand_limits(y = 1) + scale_color_manual(values = colourvector) + ggtitle("Explanatory Variables Before Normalization")
p2 <- plotExplanatoryVariables(sce_seunorm, 
                         variables = explanatory_variables) + expand_limits(y = 1) + scale_color_manual(values = colourvector) + ggtitle("Explanatory Variables After Normalization")
multiplot(p1, p2)

```


```{r}
# running PCA on the normalized counts
sce_seunorm <- runPCA(
  sce_seunorm, ncomponents = 20,
  exprs_values = "counts" 
)
```


```{r}
# plotting again the PCA's on raw-transformed and normalized values
# raw log-transformation.
tmp <- runPCA(sce_endo, ncomponents = 50, exprs_values = "logcounts_raw")
# PCA plot after log2 transformation
plotPCA(tmp, 
        colour_by = lab_col,
        size_by = "total_features_by_counts")

# PCA plot after seurat normalization
plotPCA(sce_seunorm,
        colour_by = lab_col,
        size_by = "total_features_by_counts")

```


## Build unspliced assay

Select the same cells and genes as in the spliced dataset

```{r build SCE 2}
# df -> matrix -> SCE + phenodata 
cells_use <- colnames(sce_endo)
genes_use <- rownames(sce_endo)

sce_us <- SingleCellExperiment(assays = list(counts = as.matrix(unspliced.data.df)), colData = pheno_matched, rowData = rownames(unspliced.data.df))

# Adding spike-in information:
isSpike(sce_us, "ERCC") <- grepl("^ERCC-", rownames(sce_us))
isSpike(sce_us, "MT") <- grepl("^MT-", rownames(sce_us))

# Dataset after filtering:
sce_usmatch <- sce_us[genes_use,cells_use]

# Calculate the quality metrics:
sce_us <- calculateQCMetrics(
  sce_us, feature_controls = list(
    ERCC = isSpike(sce_us, "ERCC"), 
    MT = isSpike(sce_us, "MT")
    )
  )
sce_usmatch <- calculateQCMetrics(
  sce_usmatch, feature_controls = list(
    ERCC = isSpike(sce_usmatch, "ERCC"), 
    MT = isSpike(sce_usmatch, "MT")
    )
  )
# Arbitrary thresholds:
# Looking at the total number of RNA molecules per sample
# UMI counts were used for this experiment
hist(sce_us$total_counts, breaks = 100)
abline(v = total_counts_tresh, col = "red")

# Looking at the amount of unique genes per sample
# This is the amount with ERCC included.
hist(sce_us$total_features_by_counts, breaks = 100)
abline(v= total_feat_tresh, col = "red")

hist(sce_usmatch$total_counts, breaks = 100)
abline(v = total_counts_tresh, col = "red")
hist(sce_usmatch$total_features_by_counts, breaks = 100)
abline(v= total_feat_tresh, col = "red")

pdf("Histograms_before+aftercellsFiltering_UnsplicedReads.pdf")
par(mfrow=c(2,2))
hist(sce_us$total_counts, breaks = 100)
abline(v = total_counts_tresh, col = "red")
hist(sce_us$total_features_by_counts, breaks = 100)
abline(v= total_feat_tresh, col = "red")
hist(sce_usmatch$total_counts, breaks = 100)
abline(v = total_counts_tresh, col = "red")
hist(sce_usmatch$total_features_by_counts, breaks = 100)
abline(v= total_feat_tresh, col = "red")
dev.off()

```
## Build Seurat object with unspliced and spliced assay

```{r}
unspliced_match <- unspliced.data.df[genes_use,cells_use]
unspliced_match <- as.matrix(unspliced_match)

seu[["uf"]] <- CreateAssayObject(counts = unspliced_match)

seu <- NormalizeData(
    object = seu, assay = "sf",
    normalization.method = "LogNormalize", 
    scale.factor = 10000
)
seu <- NormalizeData(
    object = seu, assay = "uf",
    normalization.method = "LogNormalize", 
    scale.factor = 10000
)

```


## Highly variable genes & Scaling of the gene expression values

```{r}

# FindVariableFeatures plots the dispersion (= a normalized measure of cell-to-cell variation), as a function of average expression for each gene. 
# In their tutorial the Satija lab uses the cut-off of 2000 genes.
seu <- FindVariableFeatures(
    object = seu, assay = "sf",
    selection.method = "vst",
    nfeatures = nHVG)

seu <- FindVariableFeatures(
    object = seu, assay = "uf",
    selection.method = "vst",
    nfeatures = nHVG)

# top 10 most variable genes
top20 <- head(VariableFeatures(seu, assay = "sf"), 20)
top20_uf <- head(VariableFeatures(seu, assay = "uf"), 20)

# plot variable features with labels:
plot1 <- VariableFeaturePlot(seu)
plot2 <- LabelPoints(plot = plot1, points = top20, repel = TRUE)
plot2
plot3 <- VariableFeaturePlot(seu, assay = "uf")
plot4 <- LabelPoints(plot = plot3, points = top20_uf, repel = TRUE)
plot4

# Preferable removing the genes that are highly expressed but with a low variance.
length(x = seu@assays$sf@var.features)
seu[["sf"]]@var.features[1:10]


```

```{r}
# Scaling the data to make it usable for dimensional reduction 
# using all the genes, could also select only the highly variable genes. 
all.genes <- rownames(seuset)
seu <- ScaleData(
    object = seu,  vars.to.regress = vars_to_regress,
    assay = "sf",
    features = all.genes
)
seu <- ScaleData(
    object = seu,  vars.to.regress = vars_to_regress,
    assay = "uf",
    features = all.genes
)
```

## Running PCA analysis on the scaled data
```{r}
seuset <- seu
rm(seu)
DefaultAssay(seuset) <- "sf"
seuset <- RunPCA(
    object = seuset,
    features = VariableFeatures(object = seuset), 
    npcs = pcs_max,
    ndims.print = 1:5, 
    nfeatures.print = 5
)
length(seuset[["sf"]]@var.features)
length(seuset[["uf"]]@var.features)
```

## Visualizing PCA results:
```{r}
#PrintPCA(object = seuset.scnorm, pcs.print = 1:5, genes.print = 5, use.full = FALSE)

VizDimLoadings(object = seuset, dims = 1:10, reduction = "pca")
VizDimLoadings(object = seuset, dims = 10:20, reduction = "pca")
pdf(paste0("VizPCAplot_PCs1-", pcs_max, ".pdf"), width = 20, height = 60)
VizDimLoadings(object = seuset, dims = 1:pcs_max, reduction = "pca")
dev.off()

DimPlot(object = seuset, reduction = "pca", group.by = lab_col)

# Helping in choosing the PCs to include in the analysis
DimHeatmap(
    object = seuset, 
    dims = 1:5, 
    cells = 500, 
    balanced = TRUE
)

pdf(paste0("PCheatmap_PCs1-", pcs_max, ".pdf"), width = 20, height = 60)
DimHeatmap(
    object = seuset, 
    dims = 1:pcs_max, 
    cells = 500, 
    balanced = TRUE
)
dev.off()
```

## Perform JackStraw Permutations to find significant PCs

```{r}
# seuset.jack <- JackStraw(
#     object = seuset,
#     num.replicate = 100
# )
# seuset.jack <- ScoreJackStraw(seuset.jack, dims = 1:20)
```

```{r}
# JackStrawPlot(object = seuset.jack, dims = 1:20)
```

## Plotting Elbow plot to identify significant PCs
This plot displays the standard deviations of the PCs and the 

```{r}
ElbowPlot(object = seuset, ndims = 35)
```

## Overview of different UMAPs with varying dimensional input

```{r}
# Generating a combined plot with only a legend in the first plotted (since this will be the same for the others)
plot.list <- list()
plot.list2 <- list()
label.vector <- c(umap_col, umap_col2)
j = 0

## This could be done nicer with a loop probably, in which for each principle component of interest, there is a umap run, and 2 different labels are shown for it.

for (i in (1:length(pcs_for_overview))){
  seuset <- RunUMAP(seuset, dims = 1:pcs_for_overview[i])
  dimnr <- as.character(pcs_for_overview[i])
  print(dimnr)
  if (i == 1){
    plot.list[[dimnr]] <- DimPlot(seuset, reduction = "umap", group.by = label.vector[1], combine = TRUE) + ggtitle(paste0("UMAP 1:", dimnr))
    plot.list2[[dimnr]] <- DimPlot(seuset, reduction = "umap", group.by = label.vector[2], combine = TRUE) + ggtitle(paste0("UMAP 1:", dimnr))
    i = i + 1
  }
  else {
    plot.list[[dimnr]] <- DimPlot(seuset, reduction = "umap", group.by = label.vector[1], combine = TRUE) + ggtitle(paste0("UMAP 1:", dimnr)) + theme(legend.position = "none")
    plot.list2[[dimnr]] <- DimPlot(seuset, reduction = "umap", group.by = label.vector[2], combine = TRUE) + ggtitle(paste0("UMAP 1:", dimnr)) + theme(legend.position = "none")
  i = i + 1
  }
}

pdf(paste0("UMAPdiffsettings_", paste(as.character(pcs_for_overview), collapse = "-"), label.vector[1], ".pdf"), width = 20, height = 15)
CombinePlots(plot.list, nrows = round(length(pcs_for_overview)/3))
dev.off()

pdf(paste0("UMAPdiffsettings_", paste(as.character(pcs_for_overview), collapse = "-"), label.vector[2], ".pdf"), width = 20, height = 15)
CombinePlots(plot.list2, nrows = round(length(pcs_for_overview)/3))
dev.off()

```
Based on the heatmaps, elbow (as well as the JackStraw indicating these are significant as well) the first 6 PCs can be used for further analysis.

```{r}
# Saving the dataset with the normalized, scaled and identified HVGs (stored in seuset.scnorm@var.genes).
saveRDS(seuset, file="seusetv3_scnormHVG_velocity.rds")
```

```{r}
sessionInfo()
```

# Now use this file in the Velocyto.R dedicated Conda environment: 
conda activate kb_scrna_velocyto2

